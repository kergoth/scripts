#!/usr/bin/env -S uv run --script
#
# /// script
# requires-python = ">=3.11"
# dependencies = ["httpx", "rich"]
# ///
#
# TODO: Consider alternative release notes links. Older releases in particular
#       don't have "RELEASENOTES", but other candidates like "RELEASENOTES.txt"
#       and README
# TODO: Fetch per-release details from RELEASENOTES (commit hashes, layer info)
# TODO: Show individual point releases within a series (optionally nested in JSON)
# TODO: Consider refactoring the list of releases into some sort of Table object
#       holding both the rows and column headers. The pattern of "get a table from
#       somewhere and then print it in various tabular formats" is more generic than
#       the current implementation, which is specific to the release table.
#       We could potentially also create a standalone formatted csv output tool in
#       the same style as this script.

"""Fetch and display Yocto Project release series information.

Data source: Yocto Project wiki release table (MediaWiki raw wikitext).
"""

from __future__ import annotations

import argparse
import asyncio
import csv
import dataclasses
import io
import json
import re
import sys
from dataclasses import dataclass
from datetime import datetime

import httpx
from rich.console import Console
from rich.table import Table
from rich.text import Text

WIKI_RAW_URL = "https://wiki.yoctoproject.org/wiki/index.php?title=Releases&action=raw"
DOWNLOADS_URL_FMT = "https://downloads.yoctoproject.org/releases/yocto/yocto-{version}/"
RELEASE_NOTES_URL_FMT = (
    "https://downloads.yoctoproject.org/releases/yocto/yocto-{version}/RELEASENOTES"
)
MIGRATION_GUIDE_URL_FMT = (
    "https://docs.yoctoproject.org/dev/migration-guides/migration-{series}.html"
)

MONTHS = {
    "jan": "01",
    "january": "01",
    "feb": "02",
    "february": "02",
    "mar": "03",
    "march": "03",
    "apr": "04",
    "april": "04",
    "may": "05",
    "jun": "06",
    "june": "06",
    "jul": "07",
    "july": "07",
    "aug": "08",
    "august": "08",
    "sep": "09",
    "september": "09",
    "oct": "10",
    "october": "10",
    "nov": "11",
    "november": "11",
    "dec": "12",
    "december": "12",
}

# Used to strip out unicode superscripts (i.e. ¹, ², ³, etc.)
SUPERSCRIPTS = str.maketrans(
    "", "", "\u00b9\u00b2\u00b3\u2070\u2074\u2075\u2076\u2077\u2078\u2079"
)


@dataclass
class YoctoRelease:
    """A Yocto Project release series."""

    version: str
    codename: str
    release_date: str
    current_version: str
    current_version_date: str
    support_status: str
    support_detail: str
    eol_date: str
    bitbake_branch: str
    maintainer: str
    downloads_url: str
    release_notes_url: str
    migration_guide_url: str


DEFAULT_COLUMNS = [
    "version",
    "codename",
    "support_status",
    "bitbake_branch",
    "current_version",
    "release_date",
    "eol_date",
]

# The 'Support detail' field, while useful as a pristine version of the
# source data, provides little beyond the support status and EOL date.
# 'Maintainer', while useful, is increasing the column count beyond a
# reasonable limit.
VERBOSE_EXCLUDED_COLUMNS = {"maintainer", "support_detail"}
VERBOSE_COLUMNS = [
    f.name
    for f in dataclasses.fields(YoctoRelease)
    if f.name not in VERBOSE_EXCLUDED_COLUMNS
]

COLUMN_HEADERS = {
    "version": "Version",
    "codename": "Codename",
    "release_date": "Released",
    "current_version": "Current",
    "current_version_date": "Updated",
    "support_status": "Status",
    "support_detail": "Support Detail",
    "eol_date": "EOL Date",
    "bitbake_branch": "BitBake",
    "maintainer": "Maintainer",
    "downloads_url": "Downloads",
    "release_notes_url": "Release Notes",
    "migration_guide_url": "Migration Guide",
}


@dataclass
class WikiRow:
    """Raw cell values from a single wiki table row."""

    codename: str = ""
    version: str = ""
    date: str = ""
    current: str = ""
    support: str = ""
    poky: str = ""
    bitbake: str = ""
    maintainer: str = ""
    notes: str = ""

    @classmethod
    def from_cells(cls, cells: list[str]) -> WikiRow:
        """Construct from raw cells, padding or truncating to fit."""
        n = len(dataclasses.fields(cls))
        return cls(*(cells + [""] * n)[:n])


def fetch_wiki_text() -> str:
    """Fetch raw wikitext from the Yocto Project Releases wiki page."""
    response = httpx.get(WIKI_RAW_URL, timeout=30, follow_redirects=True)
    response.raise_for_status()
    return response.text


def extract_first_table(wikitext: str) -> str:
    """Extract the first wiki table from the wikitext."""
    start = wikitext.find("{|")
    if start == -1:
        raise ValueError("No wiki table found in page content")
    end = wikitext.find("|}", start)
    if end == -1:
        raise ValueError("Unterminated wiki table")
    return wikitext[start : end + 2]


def split_table_rows(table_text: str) -> list[list[str]]:
    """Split a wiki table into rows of cell values."""
    lines = table_text.split("\n")
    rows: list[list[str]] = []
    current_row: list[str] = []
    in_header = True

    for line in lines:
        stripped = line.strip()
        if stripped.startswith("|-") or stripped == "|}":
            if current_row and not in_header:
                rows.append(current_row)
            current_row = []
            in_header = False
        elif stripped.startswith("!"):
            continue
        elif stripped.startswith("|") and not stripped.startswith("{|"):
            current_row.append(stripped[1:].strip())

    if current_row:
        rows.append(current_row)

    return rows


def clean_cell(text: str) -> str:
    """Strip HTML tags, wiki links, and footnote markers from a cell value."""
    text = text.translate(SUPERSCRIPTS)
    text = re.sub(r"<br\s*/?>.*", "", text)
    text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r"\[\[[^|\]]*\|([^\]]+)\]\]", r"\1", text)
    text = re.sub(r"\[\[([^\]]+)\]\]", r"\1", text)
    text = re.sub(r"\[https?://\S+\s+([^\]]+)\]", r"\1", text)
    text = re.sub(r"\[https?://\S+\]", "", text)
    text = re.sub(r"'''(.+?)'''", r"\1", text)
    text = re.sub(r"''(.+?)''", r"\1", text)
    return text.strip()


def parse_codename(raw: str) -> str:
    """Extract the codename as first word, lowercased."""
    cleaned = clean_cell(raw)
    return cleaned.split()[0].lower() if cleaned else ""


def normalize_date(raw: str) -> str:
    """Normalize date strings like 'April 2024', 'Apr 2013', '11 June 2010' to YYYY-MM."""
    text = raw.strip().rstrip(".")
    text = text.translate(SUPERSCRIPTS)
    if not text:
        return ""

    parts = text.split()
    if len(parts) == 2:
        month_str, year = parts
    elif len(parts) == 3:
        _, month_str, year = parts
    else:
        month_str, year = None, None

    if month_str and year.isdigit():
        month = MONTHS.get(month_str.rstrip(".").lower(), "")
        if month:
            return f"{year}-{month}"

    return raw.strip()


def parse_current_version(raw: str) -> tuple[str, str]:
    """Parse '5.0.15 (January 2026)' into (version, normalized_date)."""
    cleaned = clean_cell(raw)
    if not cleaned:
        return "", ""

    match = re.match(r"([\d.]+)\s*\((.+?)\)", cleaned)
    if match:
        return match.group(1), normalize_date(match.group(2))

    if re.match(r"[\d.]+$", cleaned):
        return cleaned, ""

    return cleaned, ""


def normalize_support_status(raw: str) -> tuple[str, str]:
    """Return (normalized_status, original_detail) from support level text."""
    cleaned = clean_cell(raw)
    lower = cleaned.lower()

    if lower == "future":
        return "Future", cleaned
    if "long term support" in lower or (lower.startswith("support") and "lts" in lower):
        return "LTS", cleaned
    if lower.startswith("eol") and "lts" in lower:
        return "EOL (LTS)", cleaned
    if lower.startswith("eol"):
        return "EOL", cleaned
    if lower.startswith("support"):
        return "Supported", cleaned

    if not cleaned:
        return "EOL", ""
    return cleaned, cleaned


def compute_eol_date(
    release_date: str, support_status: str, support_detail: str
) -> str:
    """Compute the EOL date from the release date and support period."""
    until_match = re.search(r"until\s+(.+?)\)", support_detail, re.IGNORECASE)
    if until_match:
        return normalize_date(until_match.group(1))

    since_match = re.search(r"EOL\s+since\s+(.+)", support_detail, re.IGNORECASE)
    if since_match:
        return normalize_date(since_match.group(1))

    if not release_date or "-" not in release_date:
        return ""

    parts = release_date.split("-")
    if len(parts) < 2:
        return ""

    year, month = int(parts[0]), int(parts[1])

    if support_status in ("LTS", "EOL (LTS)"):
        return f"{year + 4}-{month:02d}"
    elif support_status in ("Supported", "EOL"):
        month += 6
        if month > 12:
            month -= 12
            year += 1
        return f"{year}-{month:02d}"

    return ""


def parse_date(date_str: str) -> datetime.date | None:
    """Parse a YYYY-MM date string into a date object, or None if unparseable."""
    try:
        return datetime.strptime(date_str, "%Y-%m").date()
    except ValueError:
        return None


def clean_maintainer(raw: str) -> str:
    """Strip email addresses, keep just the name."""
    cleaned = clean_cell(raw)
    cleaned = re.sub(r"\s*<[^>]+>", "", cleaned)
    cleaned = re.sub(r"\s*\([^)]*@[^)]*\)", "", cleaned)
    return cleaned.strip()


def make_urls(current_version: str) -> tuple[str, str, str]:
    """Construct downloads, release notes, and migration guide URLs."""
    if not current_version:
        return "", "", ""
    series = ".".join(current_version.split(".")[:2])
    return (
        DOWNLOADS_URL_FMT.format(version=current_version),
        RELEASE_NOTES_URL_FMT.format(version=current_version),
        MIGRATION_GUIDE_URL_FMT.format(series=series),
    )


def parse_release_table(wikitext: str) -> list[YoctoRelease]:
    """Parse the wiki release table into a list of YoctoRelease objects."""
    table_text = extract_first_table(wikitext)
    raw_rows = split_table_rows(table_text)
    releases: list[YoctoRelease] = []

    for cells in raw_rows:
        row = WikiRow.from_cells(cells)

        codename = parse_codename(row.codename)
        if not codename:
            continue

        version = clean_cell(row.version)
        if not version:
            continue

        release_date = normalize_date(clean_cell(row.date))
        current_version, current_version_date = parse_current_version(row.current)
        support_status, support_detail = normalize_support_status(row.support)
        bitbake_branch = clean_cell(row.bitbake)
        maintainer = clean_maintainer(row.maintainer)
        eol_date = compute_eol_date(release_date, support_status, support_detail)
        downloads_url, release_notes_url, migration_guide_url = make_urls(
            current_version or version
        )

        releases.append(
            YoctoRelease(
                version=version,
                codename=codename,
                release_date=release_date,
                current_version=current_version,
                current_version_date=current_version_date,
                support_status=support_status,
                support_detail=support_detail,
                eol_date=eol_date,
                bitbake_branch=bitbake_branch,
                maintainer=maintainer,
                downloads_url=downloads_url,
                release_notes_url=release_notes_url,
                migration_guide_url=migration_guide_url,
            )
        )

    return releases


def _linkify(url: str, label: str) -> Text | str:
    """Return a rich Text with a clickable hyperlink style, or empty string if no URL."""
    if not url:
        return ""
    return Text(label, style=f"link {url}")


def format_rich(
    releases: list[YoctoRelease], verbose: bool = False, hyperlink: bool = False
) -> Table:
    """Return a styled rich table to the terminal using rich."""
    columns = VERBOSE_COLUMNS if verbose else DEFAULT_COLUMNS
    table = Table(show_header=True, header_style="bold", show_lines=False)

    for col in columns:
        table.add_column(COLUMN_HEADERS.get(col, col))

    for release in releases:
        data = dataclasses.asdict(release)
        if hyperlink:
            data["downloads_url"] = _linkify(data["downloads_url"], "Download")
            data["release_notes_url"] = _linkify(
                data["release_notes_url"], "Release Notes"
            )
            data["migration_guide_url"] = _linkify(
                data["migration_guide_url"], "Migration Guide"
            )
        values = [data[col] for col in columns]

        style = None
        if release.support_status.startswith("EOL"):
            style = "dim"
        elif release.support_status == "LTS":
            style = "green"
        elif release.support_status == "Supported":
            style = "cyan"

        table.add_row(*values, style=style)

    return table


def format_plain(releases: list[YoctoRelease], verbose: bool = False) -> str:
    """Format releases as an aligned plain text table."""
    columns = VERBOSE_COLUMNS if verbose else DEFAULT_COLUMNS
    headers = [COLUMN_HEADERS.get(col, col) for col in columns]

    rows = []
    for release in releases:
        data = dataclasses.asdict(release)
        rows.append([data[col] for col in columns])

    widths = [len(h) for h in headers]
    for row in rows:
        for i, val in enumerate(row):
            widths[i] = max(widths[i], len(val))

    fmt = "  ".join(f"{{:<{w}}}" for w in widths)
    lines = [fmt.format(*headers), fmt.format(*("-" * w for w in widths))]
    for row in rows:
        lines.append(fmt.format(*row))

    return "\n".join(lines)


def format_csv(releases: list[YoctoRelease], delimiter: str = ",") -> str:
    """Format releases as CSV with all fields."""
    output = io.StringIO()
    fieldnames = [f.name for f in dataclasses.fields(YoctoRelease)]
    writer = csv.DictWriter(output, fieldnames=fieldnames, delimiter=delimiter)
    writer.writeheader()
    for release in releases:
        writer.writerow(dataclasses.asdict(release))
    return output.getvalue().rstrip("\n")


def format_tsv(releases: list[YoctoRelease]) -> str:
    """Format releases as TSV with all fields."""
    return format_csv(releases, delimiter="\t")


def format_json(releases: list[YoctoRelease]) -> str:
    """Format releases as JSON with all fields."""
    return json.dumps([dataclasses.asdict(r) for r in releases], indent=2)


def parse_args() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description=__doc__.strip(),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "-s",
        "--supported-only",
        action="store_true",
        help="exclude EOL releases",
    )
    parser.add_argument(
        "-l",
        "--lts-only",
        action="store_true",
        help="only show LTS releases (includes EOL LTS)",
    )
    parser.add_argument(
        "--version",
        metavar="VERSION",
        help="filter to a specific version (e.g. '5.0')",
    )
    parser.add_argument(
        "--codename",
        metavar="NAME",
        help="filter to a specific codename (e.g. 'scarthgap')",
    )
    parser.add_argument(
        "-f",
        "--format",
        choices=["rich", "plain", "csv", "tsv", "json"],
        default=None,
        help="output format (default: rich if TTY, plain otherwise)",
    )
    parser.add_argument(
        "--no-hyperlink",
        dest="hyperlink",
        action="store_false",
        help="disable hyperlinks in rich output",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="show all fields in table output",
    )
    return parser.parse_args()


def infer_future_lts(releases: list[YoctoRelease]) -> list[YoctoRelease]:
    """Tag the upcoming release as Future (LTS) if it follows the 2-year LTS cadence."""
    if not releases or releases[0].support_status != "Future":
        return releases

    lts_releases = [r for r in releases if r.support_status == "LTS"]
    if not lts_releases:
        return releases

    future_date = parse_date(releases[0].release_date)
    lts_date = parse_date(lts_releases[0].release_date)
    if not future_date or not lts_date:
        return releases

    if future_date.month == lts_date.month and future_date.year == lts_date.year + 2:
        eol_date = f"{future_date.year + 4}-{future_date.month:02d}"
        updated = dataclasses.replace(
            releases[0],
            support_status="Future (LTS)",
            support_detail=f"Future (LTS) (Until {eol_date})",
            eol_date=eol_date,
        )
        return [updated, *releases[1:]]

    return releases


async def _check_url(
    client: httpx.AsyncClient,
    url: str,
    *,
    timeout_s: float,
) -> bool:
    """
    Return True if URL is considered valid.

    Policy:
      - Follow redirects.
      - Only treat 404 as invalid (per request).
      - Be conservative on network errors/timeouts: treat as valid (avoid false negatives).
      - Use HEAD first; fall back to GET if HEAD is not usable.
    """
    if not url:
        return False

    try:
        r = await client.head(url, timeout=timeout_s)
        if r.status_code in (403, 405):
            r = await client.get(url, timeout=timeout_s)

        return r.status_code != 404
    except (httpx.TimeoutException, httpx.TransportError):
        return True


async def _validate_urls_async(
    releases: list[YoctoRelease],
    *,
    max_concurrency: int,
    timeout_s: float,
) -> list[YoctoRelease]:
    sem = asyncio.Semaphore(max_concurrency)

    async with httpx.AsyncClient(follow_redirects=True) as client:

        async def guarded_check(url: str) -> bool:
            async with sem:
                return await _check_url(client, url, timeout_s=timeout_s)

        # Flatten tasks as (release_index, field_name, url)
        work: list[tuple[int, str, str]] = []
        for i, r in enumerate(releases):
            if r.downloads_url:
                work.append((i, "downloads_url", r.downloads_url))
            if r.release_notes_url:
                work.append((i, "release_notes_url", r.release_notes_url))
            if r.migration_guide_url:
                work.append((i, "migration_guide_url", r.migration_guide_url))

        if not work:
            return releases

        results = await asyncio.gather(*(guarded_check(url) for _, _, url in work))

    updated = list(releases)
    for (i, field, url), ok in zip(work, results, strict=True):
        if not ok:
            updated[i] = dataclasses.replace(updated[i], **{field: ""})

    return updated


def validate_urls(
    releases: list[YoctoRelease],
    *,
    max_concurrency: int = 32,
    timeout_s: float = 10.0,
) -> list[YoctoRelease]:
    """
    Validate per-release URLs in parallel and blank out only those that 404.

    Returns a new releases list with updated dataclasses.
    """
    return asyncio.run(
        _validate_urls_async(
            releases,
            max_concurrency=max_concurrency,
            timeout_s=timeout_s,
        )
    )


def main() -> int:
    """Entry point."""
    args = parse_args()
    console = Console()
    err_console = Console(stderr=True)

    try:
        wikitext = fetch_wiki_text()
    except (httpx.HTTPError, httpx.TimeoutException) as exc:
        err_console.print(f"[red]Error fetching wiki data: {exc}")
        return 1

    try:
        releases = parse_release_table(wikitext)
    except ValueError as exc:
        err_console.print(f"[red]Error parsing release table: {exc}")
        return 1

    releases = infer_future_lts(releases)

    if args.lts_only:
        releases = [
            r
            for r in releases
            if r.support_status in ("Future (LTS)", "LTS", "EOL (LTS)")
        ]
    elif args.supported_only:
        releases = [r for r in releases if not r.support_status.startswith("EOL")]

    if args.version:
        releases = [r for r in releases if r.version == args.version]
        if not releases:
            err_console.print(f"[red]No release found with version {args.version}")
            return 1

    if args.codename:
        target = args.codename.lower()
        releases = [r for r in releases if r.codename == target]
        if not releases:
            err_console.print(f"[red]No release found with codename {target}")
            return 1

    if not releases:
        err_console.print(
            "[red]No releases found. This is most likely a bug in the script.",
        )
        return 1

    releases = validate_urls(releases)

    fmt = args.format
    if fmt is None:
        fmt = "rich" if sys.stdout.isatty() else "plain"

    if fmt == "rich":
        console.print(
            format_rich(releases, verbose=args.verbose, hyperlink=args.hyperlink)
        )
    elif fmt == "plain":
        console.print(format_plain(releases, verbose=args.verbose))
    elif fmt == "csv":
        console.print(format_csv(releases))
    elif fmt == "tsv":
        console.print(format_tsv(releases))
    elif fmt == "json":
        console.print(format_json(releases))

    return 0


if __name__ == "__main__":
    sys.exit(main())
